{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zE6e1uroW7rS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only sepal length and width as input features\n",
        "y = iris.data[:, 2:]  # Predict petal length and width\n"
      ],
      "metadata": {
        "id": "O3pDhn05f24u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "YVvBMasSYboM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the regression model class\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros((n_features, y.shape[1]))\n",
        "        self.bias = np.zeros(y.shape[1])\n",
        "\n",
        "        for _ in range(self.max_epochs):\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "            grad_weights = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            grad_bias = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * grad_weights\n",
        "            self.bias -= self.learning_rate * grad_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n"
      ],
      "metadata": {
        "id": "h3Zz4l-if7t-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Train the regression model\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict petal length and width for the test set\n",
        "y_pred = regression_model.predict(X_test)\n",
        "\n",
        "# Get the predicted sepal length and width\n",
        "sepal_length_pred = y_pred[:, 0]\n",
        "sepal_width_pred = y_pred[:, 1]\n"
      ],
      "metadata": {
        "id": "hRj_haA2f-H8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the predicted sepal length and width\n",
        "print(\"Predicted Sepal Length:\", sepal_length_pred)\n",
        "print(\"Predicted Sepal Width:\", sepal_width_pred)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the mean squared error\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pgVDirpgAJw",
        "outputId": "44502b84-7924-485e-d881-92787f8dd4b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sepal Length: [ 3.38603547e+00  1.05263256e+00  6.53212255e+00  3.04692913e+00\n",
            "  4.61997267e+00  1.17512071e+00  2.34182216e+00  4.30776064e+00\n",
            "  4.53928977e+00  3.02003483e+00  3.43982407e+00  7.68778622e-01\n",
            "  1.18856786e+00  7.82225772e-01 -5.02789510e-03  2.92444099e+00\n",
            "  3.76548326e+00  2.99314053e+00  2.68092850e+00  3.91486570e+00\n",
            "  2.66842692e-01  3.06037628e+00  4.70013735e-01  3.91486570e+00\n",
            "  4.93072092e+00  4.11803674e+00  4.93218471e+00  3.96865430e+00\n",
            "  7.68778622e-01  6.05949029e-01 -5.60752425e-01  7.56550045e-02\n",
            "  3.95520715e+00  1.17460248e-01 -2.61987538e-01  4.22707774e+00\n",
            "  3.26354733e+00  6.59737628e-01  1.44354548e-01 -3.17239932e-01\n",
            "  3.02003483e+00  2.23278117e+00  3.95520715e+00  3.60972741e-01\n",
            "  6.86631928e-01]\n",
            "Predicted Sepal Width: [2.71703552 2.35517961 3.87048786 2.63817568 3.21490184 2.17275268\n",
            " 2.35368064 3.26281734 2.83457581 2.51140033 2.97058622 1.77695447\n",
            " 2.23614036 1.84034214 1.92843706 2.82060261 2.98605839 2.38462499\n",
            " 2.43254048 2.9304068  1.69035854 2.70156335 1.88825764 2.9304068\n",
            " 3.91990233 3.12830591 3.16698634 3.18395749 1.77695447 1.76921839\n",
            " 1.58829043 2.30876309 3.12056982 1.74601012 1.47698726 2.88249131\n",
            " 2.89946246 2.02276908 1.87278547 1.97635255 2.51140033 2.59949524\n",
            " 3.12056982 2.13407225 2.14954442]\n",
            "Mean Squared Error: 1.885838080541918\n"
          ]
        }
      ]
    }
  ]
}